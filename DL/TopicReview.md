For review purpose. These are not solutions for problem sets. <br>

---------------------------------------------------------------

**Math/Math-related Programming**<br>

0. Math For ML https://static.us.edusercontent.com/files/fp5iS7gSfEKiqEz5OchGseaY
1. pdf to cdf example: https://blogs.ubc.ca/math105/continuous-random-variables/example/ 
2. inner products and norms: https://www.princeton.edu/~aaa/Public/Teaching/ORF523/S17/ORF523_S17_Lec2_gh.pdf 
3. gradient vector: https://acritch.com/media/math53/Critch_Math53_09Su_-_Nabla_Notation.pdf <br> https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient
4. Sigmoid derivative via chain rule: https://hausetutorials.netlify.app/posts/2019-12-01-neural-networks-deriving-the-sigmoid-derivative/#:~:text=The%20derivative%20of%20the%20sigmoid%20function%20%CF%83(x)%20is%20the,1%E2%88%92%CF%83(x). 
5. Derivative min? https://math.stackexchange.com/questions/150960/derivative-of-the-fx-y-minx-y
6. Theory of convex functions: https://www.princeton.edu/~aaa/Public/Teaching/ORF523/S16/ORF523_S16_Lec7_gh.pdf  and https://see.stanford.edu/materials/lsocoee364a/03convexfunctions.pdf and mentioned in Theory of convex functions but more math  https://math.stackexchange.com/questions/2280341/why-is-every-p-norm-convex
7. Convex Optimization Lecture Notes https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227BT/LectureNotes_EE227BT.pdf
8. Great content connecting KL Divergence and cross entropy loss https://www.youtube.com/watch?v=SxGYPqCgJWM and https://www.youtube.com/watch?v=Pwgpl9mKars
9. One page List of Derivative Rules https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf
10. Calculus cheatsheet Derivatives https://tutorial.math.lamar.edu/pdf/calculus_cheat_sheet_derivatives.pdf
11. Why midpoint-Convex and Continuous Implies Convex https://math.stackexchange.com/questions/83383/midpoint-convex-and-continuous-implies-convex
12. Law of the unconscious statistician https://math.stackexchange.com/questions/1500751/the-law-of-the-unconscious-statistician
13. A Gentle Introduction to the Jacobian for NN  https://machinelearningmastery.com/a-gentle-introduction-to-the-jacobian/#:~:text=The%20Jacobian%20matrix%20collects%20all,one%20coordinate%20space%20and%20another. and https://www.youtube.com/watch?v=AdV5w8CY3pw
14. Hessian matrix https://en.wikipedia.org/wiki/Hessian_matrix
15. Indicator function https://en.wikipedia.org/wiki/Indicator_function
16. Element-wise product https://stats.stackexchange.com/questions/533577/what-is-the-difference-between-the-dot-product-and-the-element-by-element-multip#:~:text=With%20the%20dot%20product%2C%20you,as%20the%20original%20operand%20vectors.
17. A Gentle Introduction to Continuous Functions https://machinelearningmastery.com/continuous-functions/
18. Derivative of the Softmax Function and the Categorical Cross-Entropy Loss https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1
19. what is a convolution? https://www.youtube.com/watch?v=KuXjwB4LzSA
20. Condition number https://en.wikipedia.org/wiki/Condition_number
21. Ill-conditioned curvature https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec07.pdf
22. Pooling Derivative https://leimao.github.io/blog/Max-Pooling-Backpropagation/ and https://towardsdatascience.com/forward-and-backward-propagation-of-pooling-layers-in-convolutional-neural-networks-11e36d169bec
23. Back Propagation in Convolutional Neural Networks https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199 and https://pavisj.medium.com/convolutions-and-backpropagations-46026a8f5d2c
24. Numpy Einsum https://ajcr.net/Basic-guide-to-einsum/
25. Numpy Stride https://towardsdatascience.com/advanced-numpy-master-stride-tricks-with-25-illustrated-exercises-923a9393ab20#176a

---------------------------------------------------------------

**ML/DL**<br>
1. Parametric and Nonparametric Machine Learning Algorithms https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/
2. Explicit Regularization: [Weight Decay/L2 Norm regularization](https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate), [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/), [Data Augmentation](https://www.datacamp.com/tutorial/complete-guide-data-augmentation)
3. Implicit Regularization: Early Stopping, Batch Normalization, [SGD](https://www.inference.vc/notes-on-the-origin-of-implicit-regularization-in-stochastic-gradient-descent/)
4. Saturating Gradient Problem https://datascience.stackexchange.com/questions/27665/what-is-saturating-gradient-problem
5. Why not zero-centered bad? https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation
6. "dying ReLU" problem https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks
7. The Dead Neuron https://towardsdatascience.com/neural-network-the-dead-neuron-eaa92e575748
8. all zero initialization. See the answer starts from "It's a bad idea because of 2 reasons:", not the current highest voted one. https://stats.stackexchange.com/questions/27112/danger-of-setting-all-initial-weights-to-zero-in-backpropagation
9. 1D and 3D Convolution Neural Network https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610 and https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n
10. Co-adaptation https://datascience.stackexchange.com/questions/36064/what-is-coadaptation-of-neurons-in-neural-networks
11. Equivariance v Invariance https://fabianfuchsml.github.io/equivariance1of2/ and https://datascience.stackexchange.com/questions/16060/what-is-the-difference-between-equivariant-to-translation-and-invariant-to-tr
12. SMOTE for Imbalanced Classification https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/
13. ResNet. Why? https://www.doc.ic.ac.uk/~bkainz/teaching/DL/notes/ResNet.pdf
14. Transfer Learning https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a
15. Dense layer is fully-connected layer: Intuitively, each non linear activation function can be decomposed to Taylor series thus producing a polynomial of a degree higher than 1. By stacking several dense non-linear layers (one after the other) we can create higher and higher order of polynomials. Dense layers add an interesting non-linearity property, thus they can model any mathematical function. [source](https://medium.com/datathings/dense-layers-explained-in-a-simple-way-62fe1db0ed75)
16. Ensemble Learning Methods for NN https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/
17. t-SNE with Python Example https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1
18. What's a "patch" in CNN? https://stats.stackexchange.com/questions/309308/whats-a-patch-in-cnn
19. Receptive field arithmetic for Convolutional Neural Networks https://blog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807
20. What is a multi-headed model https://stackoverflow.com/questions/56004483/what-is-a-multi-headed-model-and-what-exactly-is-a-head-in-a-model
21. Understanding Latent Space https://stats.stackexchange.com/questions/442352/what-is-a-latent-space And https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d
22. CNN Heat Maps: Saliency/Backpropagation https://glassboxmedicine.com/2019/06/21/cnn-heat-maps-saliency-backpropagation/#:~:text=Saliency%20maps%20are%20sometimes%20referred,a%20network%20has%20finished%20training.
23. CNN Heat Maps: Class Activation Mapping (CAM) https://glassboxmedicine.com/2019/06/11/cnn-heat-maps-class-activation-mapping-cam/
24. Implementation of Class Activation Map (CAM) with PyTorch https://medium.com/intelligentmachines/implementation-of-class-activation-map-cam-with-pytorch-c32f7e414923
25. steps in a PyTorch training loop https://towardsdatascience.com/the-unofficial-pytorch-optimization-loop-song-89657dd3a434
26. Understanding LSTM Networks https://colah.github.io/posts/2015-08-Understanding-LSTMs/
27. 



---------------------------------------------------------------

**Papers mentioned**<br>
1. Corbett-Davies, Sam, and Sharad Goel. "The measure and mismeasure of fairness: A critical review of fair machine learning." arXiv preprint arXiv:1808.00023 (2018).
2. Cheng, Heng-Tze, et al. "Wide & deep learning for recommender systems." Proceedings of the 1st workshop on deep learning for recommender systems. 2016.
3. Lin, Tsung-Yi, et al. "Focal loss for dense object detection." Proceedings of the IEEE international conference on computer vision. 2017.
4. Cui, Yin, et al. "Class-balanced loss based on effective number of samples." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.


---------------------------------------------------------------
**Important concepts**<br>
1. Gradient descent and properties
2. Backpropagation
3. Representational power of neural networks, auto-differentiation
4. How computation graphs work, what properties the graph requires (DAG), ordering, etc.
5. Know when functions are differentiable and not-differentiable
6. differences between parametric/non-parametric models
7. Activation functions and their properties, including relative comparisons between them
8. Initialization, Optimizers, Regularization methods and their properties, including dropout, Batch normalization, How batch sizes affects optimization
9. Supervised Pre-training
10. Convolution
11. Types of errors and what they mean
12. Effectiveness of transfer learning under certain conditions
13. Differences between different types of modern architectures (AlexNet, VGGNet, Inception Net, ResNet)
14. Convolutional layers and how they work (forward/backward)
15. Invariance vs. equivariance
16. Backprop-based visualization
17. Adversarial examples
18. Loss functions we've covered so far (including style transfer)
19. Object detection architectures
20. Receptive fields
21. Transpose convolution
