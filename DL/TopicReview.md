For my review and understanding purpose. These are not solutions for course problem sets. <br>

---------------------------------------------------------------

**Math**<br>

0. Math For ML https://static.us.edusercontent.com/files/fp5iS7gSfEKiqEz5OchGseaY
1. pdf to cdf example: https://blogs.ubc.ca/math105/continuous-random-variables/example/ 
2. inner products and norms: https://www.princeton.edu/~aaa/Public/Teaching/ORF523/S17/ORF523_S17_Lec2_gh.pdf 
3. gradient vector: https://acritch.com/media/math53/Critch_Math53_09Su_-_Nabla_Notation.pdf <br> https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient
4. Sigmoid derivative via chain rule: https://hausetutorials.netlify.app/posts/2019-12-01-neural-networks-deriving-the-sigmoid-derivative/#:~:text=The%20derivative%20of%20the%20sigmoid%20function%20%CF%83(x)%20is%20the,1%E2%88%92%CF%83(x). 
5. Derivative min? https://math.stackexchange.com/questions/150960/derivative-of-the-fx-y-minx-y
6. Theory of convex functions: https://www.princeton.edu/~aaa/Public/Teaching/ORF523/S16/ORF523_S16_Lec7_gh.pdf  and https://see.stanford.edu/materials/lsocoee364a/03convexfunctions.pdf and mentioned in Theory of convex functions but more math  https://math.stackexchange.com/questions/2280341/why-is-every-p-norm-convex
7. Convex Optimization Lecture Notes https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227BT/LectureNotes_EE227BT.pdf
8. Great content connecting KL Divergence and cross entropy loss https://www.youtube.com/watch?v=SxGYPqCgJWM and https://www.youtube.com/watch?v=Pwgpl9mKars
9. One page List of Derivative Rules https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf
10. Calculus cheatsheet Derivatives https://tutorial.math.lamar.edu/pdf/calculus_cheat_sheet_derivatives.pdf
11. Why midpoint-Convex and Continuous Implies Convex https://math.stackexchange.com/questions/83383/midpoint-convex-and-continuous-implies-convex
12. Law of the unconscious statistician https://math.stackexchange.com/questions/1500751/the-law-of-the-unconscious-statistician
13. A Gentle Introduction to the Jacobian for NN  https://machinelearningmastery.com/a-gentle-introduction-to-the-jacobian/#:~:text=The%20Jacobian%20matrix%20collects%20all,one%20coordinate%20space%20and%20another. and https://www.youtube.com/watch?v=AdV5w8CY3pw
14. Hessian matrix https://en.wikipedia.org/wiki/Hessian_matrix
15. Indicator function https://en.wikipedia.org/wiki/Indicator_function
16. Element-wise product https://stats.stackexchange.com/questions/533577/what-is-the-difference-between-the-dot-product-and-the-element-by-element-multip#:~:text=With%20the%20dot%20product%2C%20you,as%20the%20original%20operand%20vectors.
17. A Gentle Introduction to Continuous Functions https://machinelearningmastery.com/continuous-functions/
18. Derivative of the Softmax Function and the Categorical Cross-Entropy Loss https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1
19. what is a convolution? https://www.youtube.com/watch?v=KuXjwB4LzSA
20. Condition number https://en.wikipedia.org/wiki/Condition_number
21. Ill-conditioned curvature https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec07.pdf

---------------------------------------------------------------

**ML/DL**<br>
1. Parametric and Nonparametric Machine Learning Algorithms https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/
2. Explicit Regularization: [Weight Decay/L2 Norm regularization](https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate), [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/), [Data Augmentation](https://www.datacamp.com/tutorial/complete-guide-data-augmentation)
3. Implicit Regularization: Early Stopping, Batch Normalization, [SGD](https://www.inference.vc/notes-on-the-origin-of-implicit-regularization-in-stochastic-gradient-descent/)
4. Saturating Gradient Problem https://datascience.stackexchange.com/questions/27665/what-is-saturating-gradient-problem
5. Why not zero-centered bad? https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation
6. "dying ReLU" problem https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks
7. The Dead Neuron https://towardsdatascience.com/neural-network-the-dead-neuron-eaa92e575748
8. all zero initialization. See the answer starts from "It's a bad idea because of 2 reasons:", not the current highest voted one. https://stats.stackexchange.com/questions/27112/danger-of-setting-all-initial-weights-to-zero-in-backpropagation
9. Nesterov Momentum https://machinelearningmastery.com/gradient-descent-with-nesterov-momentum-from-scratch/#:~:text=Nesterov%20momentum%20is%20an%20extension,than%20the%20actual%20positions%20themselves.
10. 1D and 3D Convolution Neural Network https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610 and https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n
11. Co-adaptation https://datascience.stackexchange.com/questions/36064/what-is-coadaptation-of-neurons-in-neural-networks
12. Equivariance v Invariance https://datascience.stackexchange.com/questions/16060/what-is-the-difference-between-equivariant-to-translation-and-invariant-to-tr
13. SMOTE for Imbalanced Classification https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/
14. placeholder
