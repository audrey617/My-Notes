# Lesson outline
- [Module: UL2 - Clustering](#1)
- [Module: UL3 - Feature Selection](#2)
- [Module: UL4 - Feature Transformation](#3)
----------------------------------------------
- [Module: RL1 - Markov Decision Processes](#4)
- [Module: RL2 - Reinforcement Learning](#5)
- [Module: RL3 - Game Theory](#6)
- [Module: RL4 - Game Theory Continued](#7)

<h1 id="1">Module: UL2 - Clustering</h1>

<h1 id="2">Module: UL3 - Feature Selection</h1>

<h1 id="3">Module: UL4 - Feature Transformation</h1>

<h1 id="4">Module: RL1 - Markov Decision Processes</h1>

<h1 id="5">Module: RL2 - Reinforcement Learning</h1>

## Extra - Intro to RL, from CIS 522 [Deep Learning @ Penn](https://www.youtube.com/watch?v=cVTud58UfpQ&list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&index=2)
### CIS522.1 Intro to RL

RL: given observations and occasional rewards as the agent performs sequential actions in an environment. Compared to supervised learning and unsupervised learning, you no longer have dataset given in advance. Instead, you receive your data as the agent performs some sequential actions in an environment. That process of performing sequential actions generates some obversations accompained by rewards. So there is no label but the reward essentially tells you whether the actions that you performed were good or not.<br/><br/>

The aim of RL is to make sequential decisions in an environment. For example, driving a car. How to learn to do these things? 1) RL assumes only occasional feedback, for example, a car crash 2) RL aims to use this feedback to learn through trail and error, as cleverly as possible.<br/><br/>

Main Idea/Turn-by-turn abstraction: <br/>
Agent + environment. Agent's goal is to maximize expected rewards. <br/>
Step1) Agent receives observations(state of the environment s_t) and feedback(reward r_t) from the world. Often, agent observers features, rather than the true state. For example, the car may not observe a pedestrain is hidden behind a car. In addition, we don't always get a reward at time t as reward is only occasional. <br/>
Step2) Agent emits an action a_t into the environment.  <br/>
Step3) Loop back to Step1, gets updated state and reward<br/>

With this abstraction, the goal of RL is to learn a policy π(s): S -> A (mapping from states to actions) for acting in the environment<br/><br/>

Characteristics of RL probelms: <br/>
1) No supervision, only (occasional) rewards as feedback <br/>
2) sequential decision making. Data is generated as sequences, not i.i.d <br/>
3) Training data is generated by the learner's own behavior<br/><br/>

Key Probelms specific to RL: <br/>
1) Credit assignment: which decisions were the good/bad ones <br/>
2) Exploration VS Exploitation: Yes, trail-and-error, but how to pick what tor try?<br/><br/>

When do we no need to worry about sequential decision making: <br/>
when your system is making a single isolated decision that does not affect future decision, e.g. classficication, regression<br/>
When should we worry about sequential decision making: <br/>
1) limited supervision: you know what you want, but not how to get it <br/>
2) actions have consequences<br/><br/>

### CIS522.2 Markov Decision Processes
Toy example - grid world - Deterministic grid world vs Stochastic grid world (Details skipped)<br/><br/>
An MDP(S,A,P,R) is defined by: <br/>
1) Set of States s ∈ S. In the grid world,it would be all the different configurations of the environment <br/>
2) Set of actions a ∈ A. In the grid world,it would be N,S,E,W<br/>
3) (State) Transition Function P(s'|s,a). It's the probability of transitioning into a new state s' given s and a. Also called the dynamics model or just the model<br/>
4) Reward function R(s,a,s') or R(s) <br/>
In RL, we typically do not know the true functions P(.) or R(.), Instead we only get samples from them. So we have to learn from trail and error <br/><br/>
The Markov Property: Given the present, the future and the past are independent<br/>

<p align="center" width="100%">
    <img width="50%" src="https://github.com/audrey617/Notes/blob/main/ML/images/rladdition1.JPG?raw=true">
</p>


### CIS522.3 Solving MDPs
To solve an MDP(S,A,P,R) means to find the optimal policy π*(s): S -> A. Optimal means following this policy will maximize the total reward/utility (on average)<br/>
In RL, P and R are unknown. But first let's assume we know the whole MDP<br/>
MDP Search Trees: Each MDP state has an associated expectimax-like tree of future outcomes from various actions. <br/>
<p align="center" width="100%">
    <img width="50%" src="https://github.com/audrey617/Notes/blob/main/ML/images/rladdition2.JPG?raw=true">
</p>

Define Utility<br/>















<h1 id="6">Module: RL3 - Game Theory</h1>

<h1 id="7">Module: RL4 - Game Theory Continued</h1>
